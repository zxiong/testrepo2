---
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  annotations:
    build.appstudio.openshift.io/repo: https://github.com/zxiong/testrepo1?rev={{revision}}
    build.appstudio.redhat.com/commit_sha: '{{revision}}'
    build.appstudio.redhat.com/pull_request_number: '{{pull_request_number}}'
    build.appstudio.redhat.com/target_branch: '{{target_branch}}'
    pipelinesascode.tekton.dev/cancel-in-progress: "true"
    pipelinesascode.tekton.dev/max-keep-runs: "3"
    pipelinesascode.tekton.dev/on-cel-expression: event == "pull_request" && target_branch
      == "main"
  creationTimestamp: null
  labels:
    appstudio.openshift.io/application: test-app1
    appstudio.openshift.io/component: test-fbc-fips1
    pipelines.appstudio.openshift.io/type: build
  name: testrepo-fbc-test1-on-pull-request
spec:
  timeouts:
    pipeline: "4h"
  params:
  - name: git-url
    value: '{{source_url}}'
  - name: revision
    value: '{{revision}}'
  - name: output-image
    value: quay.io/redhat-user-workloads/zxiong-tenant/testrepo-2-test-tasl-upgrade:{{revision}}
  - name: image-expires-after
    value: 5d
  - name: dockerfile
    value: Dockerfile
  - name: NUM_BUCKETS
    value: "4"
  - name: MAX_PARALLEL
    value: "2"
  - name: ENABLE_LOAD_BALANCING
    value: "true"
  - name: PARALLEL_FETCH_LIMIT
    value: "5" 
  pipelineSpec:
    params:
    - description: Source Repository URL
      name: git-url
      type: string
    - default: ""
      description: Revision of the Source Repository
      name: revision
      type: string
    - description: Fully Qualified Output Image
      name: output-image
      type: string
    - default: .
      description: Path to the source code of an application's component from where
        to build image.
      name: path-context
      type: string
    - default: Dockerfile
      description: Path to the Dockerfile inside the context specified by parameter
        path-context
      name: dockerfile
      type: string
    - default: "false"
      description: Force rebuild image
      name: rebuild
      type: string
    - default: "false"
      description: Skip checks against built image
      name: skip-checks
      type: string
    - default: "false"
      description: Execute the build with network isolation
      name: hermetic
      type: string
    - default: ""
      description: Build dependencies to be prefetched
      name: prefetch-input
      type: string
    - default: ""
      description: Image tag expiration time, time values could be something like
        1h, 2d, 3w for hours, days, and weeks, respectively.
      name: image-expires-after
      type: string
    - default: "false"
      description: Build a source image.
      name: build-source-image
      type: string
    - default: "false"
      description: Add built image into an OCI image index
      name: build-image-index
      type: string
    - description: Number of parallel TaskRuns (buckets) to create
      name: NUM_BUCKETS
      type: string
      default: "2"
    - description: Maximum number of images to check in parallel
      name: MAX_PARALLEL
      type: string
      default: "5"
    - description: Enable size-based load balancing across buckets
      name: ENABLE_LOAD_BALANCING
      type: string
      default: "true"
    - description: Number of parallel skopeo processes when fetching image sizes
      name: PARALLEL_FETCH_LIMIT
      type: string
      default: "5"
    tasks:
    - name: prepare-images-and-buckets
      timeout: "5m"
      params:
      - name: NUM_BUCKETS
        value: $(params.NUM_BUCKETS)
      - name: ENABLE_LOAD_BALANCING
        value: $(params.ENABLE_LOAD_BALANCING)
      - name: PARALLEL_FETCH_LIMIT
        value: $(params.PARALLEL_FETCH_LIMIT)
      taskSpec:
        params:
          - name: NUM_BUCKETS
            type: string
          - name: ENABLE_LOAD_BALANCING
            type: string
          - name: PARALLEL_FETCH_LIMIT
            type: string
        results:
          - name: IMAGES_ARTIFACT
            type: string
            description: OCI reference to images artifact
          - name: BUCKET_INDICES
            type: array
            description: Array of bucket indices for matrix expansion
        volumes:
          - name: workdir
            emptyDir: {}
        stepTemplate:
          volumeMounts:
            - name: workdir
              mountPath: /var/workdir
        steps:
          # Step 1: Extract images and split into bucket files
          - name: extract-and-split-images
            image: quay.io/konflux-ci/konflux-test:v1.4.41@sha256:afea44d83043be7f528ec2cacaeb0c3b69cdafdd86a1b930957def38400f8a6c
            env:
              - name: NUM_BUCKETS
                value: $(params.NUM_BUCKETS)
              - name: ENABLE_LOAD_BALANCING
                value: $(params.ENABLE_LOAD_BALANCING)
              - name: PARALLEL_FETCH_LIMIT
                value: $(params.PARALLEL_FETCH_LIMIT)
            script: |
              #!/usr/bin/env bash
              set -euo pipefail

              echo "Extracting images from FBC..."
              mkdir -p /var/workdir/artifact-data

              # Simulate extracting images (replace with real extraction)
              # In real case, this would be from fbc-extract-images-oci-ta
              all_images=(
"registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a"
"registry.redhat.io/openshift4/ose-prom-label-proxy-rhel9@sha256:023cf36b1efe4b343dfcb26c43fd2984083f4532c6ebb1f4b7418a6e282e6f01"
"registry.redhat.io/openshift4/ose-cli-rhel9@sha256:854608d87c79214118649df5bc96bd8f03794173489075cbd19e9f0419fb378d"
"registry.redhat.io/openshift4/ose-etcd-rhel9@sha256:734a21e351fe92a2971aeaae4d26fc1c499070863c826d1129ccf536bc3a1576"
"registry.redhat.io/openshift-service-mesh/proxyv2-rhel9@sha256:fe1e78971352ddd76ee6f88aa0f4e4ac6d1298da8c960f877ae9f883c291dfd6"
"registry.redhat.io/rhaiis/vllm-rocm-rhel9@sha256:bc99ee0a29f72ff34f930c9436d41feda2ab84bb5a53f7e76a9b8dbf8c776c44"
"registry.redhat.io/rhaiis/vllm-spyre-rhel9@sha256:80ae3e435a5be2c1f117f36599103ab05357917dd6e37f0df6613cb3ac2c13ea"
"registry.redhat.io/rhel9/postgresql-16@sha256:eca2ca0aa350aaabe79487c5be49735d892f6c02561fd73b3292488cab8254e6"
"registry.redhat.io/rhel9/mariadb-105@sha256:a5a9ffd20d7436431db01156524ff28be2dcccae7251d698c48a71b2908139f5"
"registry.redhat.io/rhoai/odh-dashboard-rhel9@sha256:f23a440709859cc7dd386af9980640bae2aa86ba77e312f04bd9ae9053e17212"
"registry.redhat.io/rhoai/odh-data-science-pipelines-argo-argoexec-rhel9@sha256:9591966e5a5cf648af1c7f59c8bb6f7f4c9896ccc82a89b35ff4b636781e646c"
"registry.redhat.io/rhelai1/instructlab-nvidia-rhel9@sha256:e6e68c4cbab408b87ae76dda7590b1855e499c83f48306c0390c843b9acc1e1a"
"registry.redhat.io/rhoai/odh-data-science-pipelines-argo-workflowcontroller-rhel9@sha256:3938e5664c572c528495ca9ea5e5da9a08f6b7011cf4c0b8fef6bd46961fac12"
"registry.redhat.io/rhoai/odh-data-science-pipelines-operator-controller-rhel9@sha256:15d484fc9114459710bacfe4645f476f18819ce1850e1188472fc8a1444e793e"
"registry.redhat.io/rhoai/odh-feast-operator-rhel9@sha256:1737d8e133c4227881a60dc6040f696770ba7807df9f5163176e947f2090dc63"
"registry.redhat.io/rhoai/odh-built-in-detector-rhel9@sha256:584446e61041e01b26c6a0ab75c923a432a403ac3c510c238f49084e3d135a55"
"registry.redhat.io/rhoai/odh-feature-server-rhel9@sha256:96c347e4b570a10d08902178f396e6dbeca171e7d2d3e4d5ced80d78f61cbebb"
"registry.redhat.io/rhoai/odh-kserve-agent-rhel9@sha256:b5358114b65d3f3e847b249e52d43707e34e8bb9f60643abef7f2ee721877592"
"registry.redhat.io/rhoai/odh-guardrails-detector-huggingface-runtime-rhel9@sha256:873483f0775cf11e81510834d41bf33a6f3735e038f4ba3e1abd07154ca4b994"
"registry.redhat.io/rhoai/odh-fms-guardrails-orchestrator-rhel9@sha256:71453c28ca28b54e932306b0dc409c781b494f1424bdc366ff2514a76b9f7f9a"
"registry.redhat.io/rhoai/odh-kserve-controller-rhel9@sha256:a6e526e0e952a3b4b97b817f594168df23c91d35e004831da9a4e6c2168f7852"
"registry.redhat.io/rhoai/odh-kserve-router-rhel9@sha256:635b4d3378c795bf94ab32627d9b000b6c8e01bccf6b3e40102da9be71c050bb"
"registry.redhat.io/rhoai/odh-kube-auth-proxy-rhel9@sha256:657bc0b43fe2da3211d80dc5085f57dccb451c30b1a254b87b4876745c414d36"
"registry.redhat.io/rhoai/odh-kf-notebook-controller-rhel9@sha256:fad6197add593a2b871c9731988eedcb64eae5e4b2ab24f40bffd19b61619939"
"registry.redhat.io/rhoai/odh-kserve-storage-initializer-rhel9@sha256:8f250f0310fa2ec6c6207497b7a896373e91a65636b7ff967e1e478fd535c5fb"
"registry.redhat.io/rhoai/odh-llm-d-inference-scheduler-rhel9@sha256:f9ecd19c51df339c7238041146bab6b0b1737f90cc16c22270e556f918c6bed0"
"registry.redhat.io/rhoai/odh-kuberay-operator-controller-rhel9@sha256:6b0ab465dfaebf06e54f628dc2bc1bf3cbc7b6f5c6371419afd4575cad6bcf69"
"registry.redhat.io/rhoai/odh-llama-stack-k8s-operator-rhel9@sha256:90d29371236fe7fbda27b2ef78bb6d3010e554d8e9055e23223702db74829837"
"registry.redhat.io/rhoai/odh-llama-stack-core-rhel9@sha256:573a361dbfb41e7c1c7fa7778838a7fa7ecfb329a1593bb5efa2d5499cf32b37"
"registry.redhat.io/rhoai/odh-maas-api-rhel9@sha256:29165b6056073e3f040a8d4eab023dcc522281888f7e6db0aecd447762761115"
"registry.redhat.io/rhoai/odh-ml-pipelines-driver-rhel9@sha256:da3c13cc6a32b1f13602f1306aff5a9cb25b56a57e9a9df6f527aa22b4644131"
"registry.redhat.io/rhoai/odh-ml-pipelines-api-server-v2-rhel9@sha256:936f5a982a23ee647865667d1db05aba62cac70e898728bcf3c6f551a1b559bb"
"registry.redhat.io/rhoai/odh-ml-pipelines-persistenceagent-v2-rhel9@sha256:83a5a1c030fd5d501fdc8d7985bdb0cd1189175af733efdb771d23884254d458"
"registry.redhat.io/rhoai/odh-ml-pipelines-scheduledworkflow-v2-rhel9@sha256:5039347c2239b04d486296e24aa6f9563eea5e787f71b9f1231cf87eac89d07b"
"registry.redhat.io/rhoai/odh-mlmd-grpc-server-rhel9@sha256:2448a37e43277d16f925a6f81b1607a276ff0b5bba0fbc6cdd285fe9c5ea3969"
"registry.redhat.io/rhoai/odh-mod-arch-gen-ai-rhel9@sha256:459e5abcc3d5403ef25f53eb4c6cba75c88371c84dbf02d8cb67f478fbe624aa"
"registry.redhat.io/rhoai/odh-ml-pipelines-launcher-rhel9@sha256:c0264549c718f776b54f1ded59865ffc5a89ca1914db1cc87c21eaa2918b81f5"
"registry.redhat.io/rhoai/odh-llm-d-routing-sidecar-rhel9@sha256:f3f3ff2e39e57ad069471fd02c28a0557247cce3734df0695f88510b91d84159"
"registry.redhat.io/rhoai/odh-model-controller-rhel9@sha256:95caeb9d3e651d566fe981cf37ea32a36666d6a63891fdb82cf39dfea77825ba"
"registry.redhat.io/rhoai/odh-mod-arch-model-registry-rhel9@sha256:7a9704afb8f64b85a1a7ae7c36cff0d3b70cea01500dfa6f372f6583df34c65f"
"registry.redhat.io/rhoai/odh-model-performance-data-rhel9@sha256:eda8a51f260ea4e061b528ed2ef6f631a8680e5623ba5ef528698e71d34b3a58"
"registry.redhat.io/rhoai/odh-model-registry-operator-rhel9@sha256:7584ae97ddbd041150c3b5417b660902cf1954f2d20574cf3b0044f5fe50606b"
"registry.redhat.io/rhoai/odh-model-registry-job-async-upload-rhel9@sha256:ada78abbd6c72c947e512edbb3760a83f4a86f8eedaf1a0dc3dd1ce6afd4a01f"
"registry.redhat.io/rhoai/odh-must-gather-rhel9@sha256:bc9d0c17b17eb7beeb22af963c3d8a63ed2c8c99564834f159aaebeed5347136"
"registry.redhat.io/rhoai/odh-model-metadata-collection-rhel9@sha256:01ed5ebd2e128782a8832d9cbd42fb83656f4b1bf2c1c1775d474180f8c63b17"
"registry.redhat.io/rhoai/odh-notebook-controller-rhel9@sha256:194f257dac479a8cbee5720178ec6876c07a17be8c39a820b8db1cad4ab3df99"
"registry.redhat.io/rhoai/odh-model-registry-rhel9@sha256:6335dcb6e1db8721de187c7527c63ffc843ac478ddbb00f3ec6cadad9df09acf"
"registry.redhat.io/rhoai/odh-pipeline-runtime-datascience-cpu-py312-rhel9@sha256:aea6b0082ff9a1c106d563c417c206860731b8c477d399459d36ee29f7724c13"
"registry.redhat.io/rhoai/odh-openvino-model-server-rhel9@sha256:a5c1453deda40218c2b359585a40fb5db61c1f077f8504d9647d1112dfdd6bef"
"registry.redhat.io/rhoai/odh-pipeline-runtime-pytorch-cuda-py312-rhel9@sha256:72a7fba56258cafa21d010064084b3814b933f119fbf38da3f74b83f831f2833"
"registry.redhat.io/rhoai/odh-pipeline-runtime-tensorflow-rocm-py312-rhel9@sha256:699db6dca8e137aa5983d4acfb083cdbd23217d99d2404a05242e7dd3fb6f901"
"registry.redhat.io/rhoai/odh-pipeline-runtime-pytorch-rocm-py312-rhel9@sha256:90c58fe78a9d28ae81ccea674d7bbc95fb548e834ea03fae3fd43c337d4c208e"
"registry.redhat.io/rhoai/odh-pipeline-runtime-pytorch-llmcompressor-cuda-py312-rhel9@sha256:1c34e22873250acbae66575a8839ccccf98785f454a69de7765e9a8652f7597a"
"registry.redhat.io/rhoai/odh-pipeline-runtime-minimal-cpu-py312-rhel9@sha256:ffdfe379ec6343b7732c7d5c0f942bfc600287b00ff3ced928f805a5e20929d9"
"registry.redhat.io/rhoai/odh-pipeline-runtime-tensorflow-cuda-py312-rhel9@sha256:2fbfc6b727833bce1981ba2a8a436be8e6cfe5f3da3321fc4253737aa6821a6b"
"registry.redhat.io/rhoai/odh-ta-lmes-driver-rhel9@sha256:613dadd6d09028f36fe8fcf7ab81b4555cb3c141a0e6bf15b789b506a1324e3a"
"registry.redhat.io/rhoai/odh-training-cuda121-torch24-py311-rhel9@sha256:38bd8a7f22c139a491b6aaa4e70a45d2eb7bfbece0ce816ae3c0e4696d1e87f9"
"registry.redhat.io/rhoai/odh-rhel9-operator@sha256:f33fd7aff26767397f08f85f3d0de782b5507e52caff606e3191b731549ddfa3"
"registry.redhat.io/rhoai/odh-training-cuda124-torch25-py311-rhel9@sha256:6c201c3461150a083b8529133e186785d8b446c933ab6d7cb50cbfc304ddf377"
"registry.redhat.io/rhoai/odh-training-cuda128-torch28-py312-rhel9@sha256:851f2b31fa418d2eb172ddfa6010851bd5d4f0844d16b2b42a408f2c0b985b86"
"registry.redhat.io/rhoai/odh-training-operator-rhel9@sha256:3a1296f264ee7172029a10fb3530f98421740e4c1595e6abedf15c2289cc9bd4"
"registry.redhat.io/rhoai/odh-training-rocm62-torch24-py311-rhel9@sha256:38568cb655ff565b32ce92a735b9e2ff802f86f08837c260a854d039954a8b54"
"registry.redhat.io/rhoai/odh-ta-lmes-job-rhel9@sha256:a5735284685521d8022f6852a1a3816b931d75dc27875a7929cb263d44c71b6c"
"registry.redhat.io/rhoai/odh-training-rocm62-torch25-py311-rhel9@sha256:e950fc961dd81bae356a5ae93cdd00c4b50799fef0ce742d92b3feeae08dcc40"
"registry.redhat.io/rhoai/odh-training-rocm64-torch28-py312-rhel9@sha256:501794be7653f12539fc420394c5ecfa2986109ef71785534e6e068212d07c25"
"registry.redhat.io/rhoai/odh-trustyai-service-rhel9@sha256:5e32b6ca3d264bdef5cc3dfc8a87faa0df77e1b46b8bfa32248db84aebe4aadc"
"registry.redhat.io/rhoai/odh-trustyai-service-operator-rhel9@sha256:fdaabbb0f18ffe3f756269946ae25dede89b333f7d464af10907bcfa82b7cf29"
"registry.redhat.io/rhoai/odh-trustyai-vllm-orchestrator-gateway-rhel9@sha256:4676682b59c63b1d3e843969f7b1afef9df7a00a898112c527052b37f3709ba4"
"registry.redhat.io/rhoai/odh-workbench-codeserver-datascience-cpu-py312-rhel9@sha256:dacc100e78ac27ee4264959ce5ab0df1a019ac2d4a85d5eb02146a52e0e12214"
"registry.redhat.io/rhoai/odh-vllm-gaudi-rhel9@sha256:89f095a5578ce33700c3a0b488d80c217c845ae5a9f57a9ea7d8e8669b255f48"
"registry.redhat.io/rhoai/odh-workbench-jupyter-pytorch-cuda-py312-rhel9@sha256:24724d9048833082445f2967d4bb7872741e72c98193c8090ff090c1a5ce4624"
"registry.redhat.io/rhoai/odh-workbench-jupyter-datascience-cpu-py312-rhel9@sha256:6aad6093e4b9398b9cb98a9a212e25b60c1502675f65daecf63c8cce7bdc142b"
"registry.redhat.io/rhoai/odh-workbench-jupyter-minimal-cpu-py312-rhel9@sha256:8b4ff98fc0eb16e34ae324599539dc0869c1ac99115d282b310f7cdfa9aba09a"
"registry.redhat.io/rhoai/odh-workbench-jupyter-minimal-rocm-py312-rhel9@sha256:5384ea03ef583ec166abab2d8cf3528cc5f25fcd23a45eb602c565678f0f11dd"
"registry.redhat.io/rhoai/odh-workbench-jupyter-minimal-cuda-py312-rhel9@sha256:c11a7953656afb0130855e9aa4ca1724e644ddd2d8702c4f525eb815d60fbb4d"
"registry.redhat.io/rhoai/odh-workbench-jupyter-pytorch-rocm-py312-rhel9@sha256:c8a8282d199322053bd652cd3e1b8df7d3307f49691d9fa9bb89fa980d55d7d4"
"registry.redhat.io/rhoai/odh-workbench-jupyter-pytorch-llmcompressor-cuda-py312-rhel9@sha256:53f63e0277f09c33b95b8b411ffb922d1bee539fe72f41656fa7680821f8f07b"
"registry.redhat.io/rhoai/odh-workbench-jupyter-tensorflow-rocm-py312-rhel9@sha256:a2acf7810ce0be4db4d49e3ef41d3311495326927c096329a0b30c7738ae8230"
"registry.redhat.io/ubi9/toolbox@sha256:88852fc9b83bd5c1dce6c8666b87830e467e3821f73a695aaf305c1cf266b490"
"registry.redhat.io/rhoai/odh-workbench-jupyter-tensorflow-cuda-py312-rhel9@sha256:28d348f1fb90589ccf7e34cc2709f8fead596440d56e4f50fe7d888bf710ffc0"
"registry.redhat.io/ubi9/python-312@sha256:e58da77627d3cd889c1fe862bac08f96da1698d0cff8e5e233d67aa9fd0083b5"
"registry.redhat.io/rhoai/odh-workbench-jupyter-trustyai-cpu-py312-rhel9@sha256:61a7338250b1aefa5bd6993a20422eacaef4bc4e41e1156843bd92e9baf3f52b"
              )
              
              num_images=${#all_images[@]}
              num_buckets=${NUM_BUCKETS}

              echo "Extracted ${num_images} images"
              echo "Splitting into ${num_buckets} buckets..."

              # Adjust num_buckets if more buckets than images
              if [ "${num_buckets}" -gt "${num_images}" ]; then
                num_buckets=${num_images}
                echo "Adjusted buckets to match images: ${num_buckets}"
              fi

              # Create empty bucket files
              for ((i=0; i<num_buckets; i++)); do
                bucket_file="/var/workdir/artifact-data/bucket-${i}.txt"
                > "${bucket_file}"  # Create empty file
              done

              # Apply load balancing if enabled
              if [[ "${ENABLE_LOAD_BALANCING}" == "true" ]]; then
                echo "Load balancing enabled: Fetching image sizes for balanced distribution..."
                echo "Fetching sizes in parallel (${PARALLEL_FETCH_LIMIT} concurrent processes)..."

                # Get image sizes in parallel
                declare -A image_sizes
                temp_sizes=$(mktemp)

                # Function to fetch single image size
                fetch_size() {
                  local img=$1
                  local size
                  # Sum all layer sizes from LayersData to get total image size
                  size=$(skopeo inspect "docker://${img}" 2>/dev/null | jq '[.LayersData[]?.Size // 0] | add // 0' || echo "0")
                  echo "${img}|${size}"
                }
                export -f fetch_size

                # Fetch sizes in parallel
                printf '%s\n' "${all_images[@]}" | xargs -P "${PARALLEL_FETCH_LIMIT}" -I {} bash -c 'fetch_size "$@"' _ {} > "${temp_sizes}"

                # Read results into associative array
                while IFS='|' read -r img size; do
                  image_sizes["$img"]=$size
                  echo "Image $img size: $size bytes"
                done < "${temp_sizes}"

                rm -f "${temp_sizes}"

                # Create array of image|size pairs
                image_list=()
                for img in "${all_images[@]}"; do
                  image_list+=("${img}|${image_sizes[$img]}")
                done

                # Sort by size (descending)
                readarray -t sorted_images < <(printf '%s\n' "${image_list[@]}" | sort -t'|' -k2 -rn)

                echo "Sorted images by size (largest first):"
                for item in "${sorted_images[@]}"; do
                  echo "  ${item}"
                done

                # Distribute using greedy algorithm (assign each image to lightest bucket)
                declare -A bucket_weights
                for ((i=0; i<num_buckets; i++)); do
                  bucket_weights[$i]=0
                done

                for item in "${sorted_images[@]}"; do
                  img="${item%%|*}"
                  size="${item##*|}"

                  # Find bucket with minimum weight
                  min_bucket=0
                  min_weight=${bucket_weights[0]}
                  for ((i=1; i<num_buckets; i++)); do
                    if [ "${bucket_weights[$i]}" -lt "${min_weight}" ]; then
                      min_weight=${bucket_weights[$i]}
                      min_bucket=$i
                    fi
                  done

                  # Add image to lightest bucket (write directly to file)
                  bucket_file="/var/workdir/artifact-data/bucket-${min_bucket}.txt"
                  echo "${img}" >> "${bucket_file}"
                  bucket_weights[$min_bucket]=$((bucket_weights[$min_bucket] + size))
                  echo "Assigned $img (size: $size) to bucket $min_bucket (total weight: ${bucket_weights[$min_bucket]})"
                done

                echo ""
                echo "Load balanced bucket distribution:"
                for ((i=0; i<num_buckets; i++)); do
                  echo "Bucket $i weight: ${bucket_weights[$i]} bytes"
                done

              else
                echo "Load balancing disabled: Using round-robin distribution..."

                # Distribute images into buckets (round-robin, write directly to files)
                bucket_idx=0
                for img in "${all_images[@]}"; do
                  bucket_file="/var/workdir/artifact-data/bucket-${bucket_idx}.txt"
                  echo "${img}" >> "${bucket_file}"
                  bucket_idx=$(( (bucket_idx + 1) % num_buckets ))
                done
              fi

              # Report bucket sizes
              echo ""
              echo "Bucket distribution complete:"
              for ((i=0; i<num_buckets; i++)); do
                bucket_file="/var/workdir/artifact-data/bucket-${i}.txt"
                bucket_size=$(wc -l < "${bucket_file}")
                echo "Bucket ${i}: ${bucket_size} images"
              done

              echo "Split complete!"
              echo "Bucket files created:"
              ls -lh /var/workdir/artifact-data/

          # Step 2: Push as OCI trusted artifact
          - name: create-trusted-artifact
            image: quay.io/konflux-ci/build-trusted-artifacts:latest@sha256:aa601d847eafa87747894b770eff43b47cffe2cc39059bb345ee58b378473b8f
            args:
              - create
              - --store
              - $(params.output-image)-fips-images
              - $(results.IMAGES_ARTIFACT.path)=/var/workdir/artifact-data

          # Step 3: Generate bucket indices
          - name: generate-bucket-indices
            image: quay.io/konflux-ci/konflux-test:v1.4.41@sha256:afea44d83043be7f528ec2cacaeb0c3b69cdafdd86a1b930957def38400f8a6c
            env:
              - name: NUM_BUCKETS
                value: $(params.NUM_BUCKETS)
            script: |
              #!/usr/bin/env bash
              set -euo pipefail

              # The artifact reference was already written by build-trusted-artifacts
              # Just verify it exists
              if [ -f "$(results.IMAGES_ARTIFACT.path)" ]; then
                artifact_ref=$(cat "$(results.IMAGES_ARTIFACT.path)")
                echo "Trusted artifact created: ${artifact_ref}"
              else
                echo "ERROR: IMAGES_ARTIFACT result not found"
                exit 1
              fi

              # Generate bucket indices
              echo "Generating bucket indices..."
              num_buckets=${NUM_BUCKETS}

              indices_json="["
              for ((i=0; i<num_buckets; i++)); do
                if [ $i -gt 0 ]; then
                  indices_json+=","
                fi
                indices_json+="\"${i}\""
              done
              indices_json+="]"

              echo "Generated bucket indices: ${indices_json}"
              echo -n "${indices_json}" > "$(results.BUCKET_INDICES.path)"

    #
    # TASK 2: Matrix FIPS check - pulls artifact and processes bucket
    #
    - name: fips-check-buckets
      timeout: "4h"
      runAfter:
        - prepare-images-and-buckets
      params:
      - name: IMAGES_ARTIFACT
        value: "$(tasks.prepare-images-and-buckets.results.IMAGES_ARTIFACT)"
      - name: TARGET_OCP_VERSION
        value: ""
      - name: MAX_PARALLEL
        value: $(params.MAX_PARALLEL)
      matrix:
        params:
          - name: BUCKET_INDEX
            value: $(tasks.prepare-images-and-buckets.results.BUCKET_INDICES[*])
      taskSpec:
        params:
          - name: BUCKET_INDEX
            type: string
          - name: IMAGES_ARTIFACT
            type: string
          - name: TARGET_OCP_VERSION
            type: string
          - name: MAX_PARALLEL
            type: string
        volumes:
          - name: workdir
            emptyDir: {}
        stepTemplate:
          volumeMounts:
            - name: workdir
              mountPath: /var/workdir
        steps:
          # Step 1: Create directory for artifact extraction
          - name: prepare-workdir
            image: quay.io/konflux-ci/konflux-test:v1.4.41@sha256:afea44d83043be7f528ec2cacaeb0c3b69cdafdd86a1b930957def38400f8a6c
            script: |
              #!/usr/bin/env bash
              set -euo pipefail
              mkdir -p /var/workdir/artifact-data
              echo "Created /var/workdir/artifact-data directory"

          # Step 2: Pull the OCI artifact
          - name: use-trusted-artifact
            image: quay.io/konflux-ci/build-trusted-artifacts:latest@sha256:aa601d847eafa87747894b770eff43b47cffe2cc39059bb345ee58b378473b8f
            args:
              - use
              - $(params.IMAGES_ARTIFACT)=/var/workdir/artifact-data

          # Step 3: Read bucket file and prepare input files for FIPS check
          - name: prepare-bucket-images
            image: quay.io/konflux-ci/konflux-test:v1.4.41@sha256:afea44d83043be7f528ec2cacaeb0c3b69cdafdd86a1b930957def38400f8a6c
            env:
              - name: BUCKET_INDEX
                value: $(params.BUCKET_INDEX)
              - name: TARGET_OCP_VERSION
                value: $(params.TARGET_OCP_VERSION)
            script: |
              #!/usr/bin/env bash
              set -euo pipefail

              echo "=========================================="
              echo "FIPS Check - Bucket ${BUCKET_INDEX}"
              echo "=========================================="

              # Read bucket file directly
              bucket_file="/var/workdir/artifact-data/bucket-${BUCKET_INDEX}.txt"

              if [ ! -f "${bucket_file}" ]; then
                echo "ERROR: Bucket file not found at ${bucket_file}"
                echo "Available files:"
                ls -la /var/workdir/artifact-data/ || true
                exit 1
              fi

              # Read images from bucket file
              readarray -t bucket_images < "${bucket_file}"
              num_bucket_images=${#bucket_images[@]}

              echo "Bucket file: ${bucket_file}"
              echo "This bucket contains: ${num_bucket_images} images"

              # Write images to file for FIPS check stepaction
              # IMPORTANT: stepaction expects space-separated images (uses mapfile -d ' ')
              mkdir -p /tekton/home
              printf '%s ' "${bucket_images[@]}" > /tekton/home/unique_related_images.txt

              # Write target OCP version if provided
              if [ -n "${TARGET_OCP_VERSION}" ]; then
                echo -n "${TARGET_OCP_VERSION}" > /tekton/home/target_ocp_version.txt
                echo "Target OCP version: ${TARGET_OCP_VERSION}"
              fi

              echo "Prepared ${num_bucket_images} images for FIPS compliance check"
              echo "========================================="

          # Step 4: Run FIPS operator check
          - name: fips-operator-check-step-action
            params:
              - name: MAX_PARALLEL
                value: $(params.MAX_PARALLEL)
            computeResources:
              limits:
                cpu: "2"
                memory: 8Gi
              requests:
                cpu: "2"
                memory: 8Gi
            ref:
              params:
                - name: url
                  #value: https://github.com/konflux-ci/build-definitions
                  value: https://github.com/zxiong/build-definitions
                - name: revision
                  #value: ac78bc7b6ea4d34781a4cff44ae9647745e2e65d
                  value: test-fips-operator-check-step-action-oc-extract
                - name: pathInRepo
                  value: stepactions/fips-operator-check-step-action/0.1/fips-operator-check-step-action.yaml
              resolver: git    
    workspaces:
    - name: git-auth
      optional: true
    - name: netrc
      optional: true
  taskRunTemplate:
    serviceAccountName: build-pipeline-testrepo-2-test-tasl-upgrade
  workspaces:
  - name: git-auth
    secret:
      secretName: '{{ git_auth_secret }}'
status: {}
